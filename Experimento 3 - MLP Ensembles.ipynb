{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#importando as bibliotecas necessárias para o experimento:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redesneuraiscin.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Os datasets para o ensemble:\n",
    "datasets = utils.ensembles_datasets_split()\n",
    "test = pd.read_csv(\"data/testDefault\", sep=\"\\t\")\n",
    "X_test, Y_test = test.iloc[:, 2:-2].values, test.iloc[:, -1].values\n",
    "X_train1, Y_train1, X_val1, Y_val1 = datasets[0]\n",
    "X_train2, Y_train2, X_val2, Y_val2 = datasets[1]\n",
    "X_train3, Y_train3, X_val3, Y_val3 = datasets[2]\n",
    "X_train4, Y_train4, X_val4, Y_val4 = datasets[3]\n",
    "X_train5, Y_train5, X_val5, Y_val5 = datasets[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 128s 503us/step - loss: 0.2558 - acc: 0.6169 - val_loss: 0.2343 - val_acc: 0.6236\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 14s 55us/step - loss: 0.2327 - acc: 0.6280 - val_loss: 0.2316 - val_acc: 0.6282\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 14s 54us/step - loss: 0.2312 - acc: 0.6310 - val_loss: 0.2310 - val_acc: 0.6310\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 14s 54us/step - loss: 0.2302 - acc: 0.6318 - val_loss: 0.2309 - val_acc: 0.6300\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.2299 - acc: 0.6331 - val_loss: 0.2298 - val_acc: 0.6319\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2294 - acc: 0.6336 - val_loss: 0.2302 - val_acc: 0.6305\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.2292 - acc: 0.6338 - val_loss: 0.2308 - val_acc: 0.6281\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.2290 - acc: 0.6339 - val_loss: 0.2297 - val_acc: 0.6316\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 14s 57us/step - loss: 0.2288 - acc: 0.6355 - val_loss: 0.2298 - val_acc: 0.6318\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 13s 53us/step - loss: 0.2287 - acc: 0.6334 - val_loss: 0.2296 - val_acc: 0.6327\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 14s 55us/step - loss: 0.2284 - acc: 0.6341 - val_loss: 0.2291 - val_acc: 0.6329\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 13s 53us/step - loss: 0.2284 - acc: 0.6349 - val_loss: 0.2299 - val_acc: 0.6302\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 14s 54us/step - loss: 0.2284 - acc: 0.6340 - val_loss: 0.2323 - val_acc: 0.6219\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2284 - acc: 0.6341 - val_loss: 0.2306 - val_acc: 0.6285\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.2283 - acc: 0.6349 - val_loss: 0.2287 - val_acc: 0.6325\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.2282 - acc: 0.6340 - val_loss: 0.2300 - val_acc: 0.6315\n",
      "Epoch 17/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.2282 - acc: 0.6354 - val_loss: 0.2289 - val_acc: 0.6322\n",
      "Epoch 18/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.2282 - acc: 0.6340 - val_loss: 0.2382 - val_acc: 0.6057\n",
      "Epoch 19/100000\n",
      "255098/255098 [==============================] - 17s 67us/step - loss: 0.2283 - acc: 0.6335 - val_loss: 0.2292 - val_acc: 0.6306\n",
      "Epoch 20/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.2282 - acc: 0.6338 - val_loss: 0.2343 - val_acc: 0.6146\n",
      "Epoch 21/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.2282 - acc: 0.6342 - val_loss: 0.2303 - val_acc: 0.6306\n",
      "Epoch 22/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.2280 - acc: 0.6343 - val_loss: 0.2289 - val_acc: 0.6291\n",
      "Epoch 23/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.2280 - acc: 0.6339 - val_loss: 0.2288 - val_acc: 0.6314\n",
      "Epoch 24/100000\n",
      "255098/255098 [==============================] - 17s 67us/step - loss: 0.2279 - acc: 0.6346 - val_loss: 0.2302 - val_acc: 0.6264\n",
      "Epoch 25/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.2281 - acc: 0.6336 - val_loss: 0.2287 - val_acc: 0.6324\n",
      "Epoch 26/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.2280 - acc: 0.6335 - val_loss: 0.2285 - val_acc: 0.6326\n",
      "Epoch 27/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.2278 - acc: 0.6347 - val_loss: 0.2287 - val_acc: 0.6311\n",
      "Epoch 28/100000\n",
      "255098/255098 [==============================] - 18s 70us/step - loss: 0.2279 - acc: 0.6341 - val_loss: 0.2281 - val_acc: 0.6326\n",
      "Epoch 29/100000\n",
      "255098/255098 [==============================] - 17s 68us/step - loss: 0.2278 - acc: 0.6350 - val_loss: 0.2282 - val_acc: 0.6324\n",
      "Epoch 30/100000\n",
      "255098/255098 [==============================] - 17s 68us/step - loss: 0.2278 - acc: 0.6341 - val_loss: 0.2282 - val_acc: 0.6304\n",
      "Epoch 31/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.2277 - acc: 0.6350 - val_loss: 0.2283 - val_acc: 0.6327\n",
      "Epoch 32/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2277 - acc: 0.6345 - val_loss: 0.2289 - val_acc: 0.6319\n",
      "Epoch 33/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2278 - acc: 0.6345 - val_loss: 0.2291 - val_acc: 0.6307\n",
      "Epoch 34/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.2277 - acc: 0.6342 - val_loss: 0.2286 - val_acc: 0.6313\n",
      "Epoch 35/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2278 - acc: 0.6345 - val_loss: 0.2287 - val_acc: 0.6317\n",
      "Epoch 36/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.2278 - acc: 0.6336 - val_loss: 0.2288 - val_acc: 0.6292\n",
      "Epoch 37/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.2277 - acc: 0.6338 - val_loss: 0.2331 - val_acc: 0.6182\n",
      "Epoch 38/100000\n",
      "255098/255098 [==============================] - 15s 61us/step - loss: 0.2276 - acc: 0.6343 - val_loss: 0.2285 - val_acc: 0.6303\n",
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 46s 179us/step - loss: 0.2627 - acc: 0.6137 - val_loss: 0.2366 - val_acc: 0.6195\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.2346 - acc: 0.6225 - val_loss: 0.2333 - val_acc: 0.6262\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2335 - acc: 0.6237 - val_loss: 0.2324 - val_acc: 0.6254\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2325 - acc: 0.6255 - val_loss: 0.2330 - val_acc: 0.6236\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2323 - acc: 0.6250 - val_loss: 0.2314 - val_acc: 0.6276\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2319 - acc: 0.6260 - val_loss: 0.2397 - val_acc: 0.6131\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2318 - acc: 0.6276 - val_loss: 0.2303 - val_acc: 0.6303\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.2314 - acc: 0.6286 - val_loss: 0.2296 - val_acc: 0.6323\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2314 - acc: 0.6284 - val_loss: 0.2299 - val_acc: 0.6324\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2310 - acc: 0.6294 - val_loss: 0.2305 - val_acc: 0.6295\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2309 - acc: 0.6290 - val_loss: 0.2300 - val_acc: 0.6310\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2307 - acc: 0.6307 - val_loss: 0.2302 - val_acc: 0.6306\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.2307 - acc: 0.6302 - val_loss: 0.2307 - val_acc: 0.6276\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.2305 - acc: 0.6300 - val_loss: 0.2317 - val_acc: 0.6289\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 18s 70us/step - loss: 0.2305 - acc: 0.6300 - val_loss: 0.2296 - val_acc: 0.6326\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.2306 - acc: 0.6307 - val_loss: 0.2296 - val_acc: 0.6333\n",
      "Epoch 17/100000\n",
      "255098/255098 [==============================] - 18s 71us/step - loss: 0.2304 - acc: 0.6305 - val_loss: 0.2290 - val_acc: 0.6335\n",
      "Epoch 18/100000\n",
      "255098/255098 [==============================] - 19s 75us/step - loss: 0.2305 - acc: 0.6299 - val_loss: 0.2292 - val_acc: 0.6328\n",
      "Epoch 19/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2303 - acc: 0.6310 - val_loss: 0.2298 - val_acc: 0.6322\n",
      "Epoch 20/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.2303 - acc: 0.6308 - val_loss: 0.2287 - val_acc: 0.6342\n",
      "Epoch 21/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.2302 - acc: 0.6316 - val_loss: 0.2290 - val_acc: 0.6333\n",
      "Epoch 22/100000\n",
      "255098/255098 [==============================] - 18s 69us/step - loss: 0.2305 - acc: 0.6311 - val_loss: 0.2288 - val_acc: 0.6353\n",
      "Epoch 23/100000\n",
      "255098/255098 [==============================] - 18s 72us/step - loss: 0.2302 - acc: 0.6318 - val_loss: 0.2298 - val_acc: 0.6319\n",
      "Epoch 24/100000\n",
      "255098/255098 [==============================] - 19s 74us/step - loss: 0.2303 - acc: 0.6307 - val_loss: 0.2298 - val_acc: 0.6339\n",
      "Epoch 25/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.2300 - acc: 0.6315 - val_loss: 0.2305 - val_acc: 0.6306\n",
      "Epoch 26/100000\n",
      "255098/255098 [==============================] - 18s 70us/step - loss: 0.2303 - acc: 0.6311 - val_loss: 0.2304 - val_acc: 0.6328\n",
      "Epoch 27/100000\n",
      "255098/255098 [==============================] - 17s 68us/step - loss: 0.2301 - acc: 0.6309 - val_loss: 0.2321 - val_acc: 0.6265\n",
      "Epoch 28/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.2304 - acc: 0.6302 - val_loss: 0.2326 - val_acc: 0.6276\n",
      "Epoch 29/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.2305 - acc: 0.6306 - val_loss: 0.2294 - val_acc: 0.6334\n",
      "Epoch 30/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.2300 - acc: 0.6312 - val_loss: 0.2365 - val_acc: 0.6161\n",
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 20s 79us/step - loss: 0.2509 - acc: 0.6145 - val_loss: 0.2382 - val_acc: 0.6085\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.2324 - acc: 0.6216 - val_loss: 0.2297 - val_acc: 0.6283\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2302 - acc: 0.6259 - val_loss: 0.2292 - val_acc: 0.6272\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 17s 67us/step - loss: 0.2292 - acc: 0.6282 - val_loss: 0.2296 - val_acc: 0.6261\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 18s 69us/step - loss: 0.2288 - acc: 0.6284 - val_loss: 0.2296 - val_acc: 0.6268\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.2284 - acc: 0.6301 - val_loss: 0.2279 - val_acc: 0.6329\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.2282 - acc: 0.6300 - val_loss: 0.2281 - val_acc: 0.6322\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.2279 - acc: 0.6313 - val_loss: 0.2316 - val_acc: 0.6237\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 17s 67us/step - loss: 0.2279 - acc: 0.6305 - val_loss: 0.2275 - val_acc: 0.6330\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.2278 - acc: 0.6310 - val_loss: 0.2272 - val_acc: 0.6342\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2276 - acc: 0.6310 - val_loss: 0.2267 - val_acc: 0.6350\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 13s 50us/step - loss: 0.2276 - acc: 0.6321 - val_loss: 0.2276 - val_acc: 0.6317\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 13s 50us/step - loss: 0.2275 - acc: 0.6320 - val_loss: 0.2270 - val_acc: 0.6357\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 13s 50us/step - loss: 0.2275 - acc: 0.6314 - val_loss: 0.2298 - val_acc: 0.6288\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 13s 52us/step - loss: 0.2276 - acc: 0.6322 - val_loss: 0.2303 - val_acc: 0.6279\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 14s 55us/step - loss: 0.2274 - acc: 0.6329 - val_loss: 0.2286 - val_acc: 0.6307\n",
      "Epoch 17/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.2275 - acc: 0.6328 - val_loss: 0.2275 - val_acc: 0.6340\n",
      "Epoch 18/100000\n",
      "255098/255098 [==============================] - 13s 49us/step - loss: 0.2274 - acc: 0.6326 - val_loss: 0.2285 - val_acc: 0.6301\n",
      "Epoch 19/100000\n",
      "255098/255098 [==============================] - 13s 51us/step - loss: 0.2274 - acc: 0.6322 - val_loss: 0.2270 - val_acc: 0.6345\n",
      "Epoch 20/100000\n",
      "255098/255098 [==============================] - 14s 55us/step - loss: 0.2274 - acc: 0.6319 - val_loss: 0.2265 - val_acc: 0.6367\n",
      "Epoch 21/100000\n",
      "255098/255098 [==============================] - 14s 54us/step - loss: 0.2273 - acc: 0.6332 - val_loss: 0.2281 - val_acc: 0.6301\n",
      "Epoch 22/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.2273 - acc: 0.6333 - val_loss: 0.2264 - val_acc: 0.6342\n",
      "Epoch 23/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2272 - acc: 0.6321 - val_loss: 0.2299 - val_acc: 0.6247\n",
      "Epoch 24/100000\n",
      "255098/255098 [==============================] - 15s 61us/step - loss: 0.2273 - acc: 0.6321 - val_loss: 0.2280 - val_acc: 0.6337\n",
      "Epoch 25/100000\n",
      "255098/255098 [==============================] - 14s 54us/step - loss: 0.2271 - acc: 0.6332 - val_loss: 0.2289 - val_acc: 0.6312\n",
      "Epoch 26/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.2271 - acc: 0.6327 - val_loss: 0.2271 - val_acc: 0.6340\n",
      "Epoch 27/100000\n",
      "255098/255098 [==============================] - 13s 52us/step - loss: 0.2271 - acc: 0.6327 - val_loss: 0.2262 - val_acc: 0.6355\n",
      "Epoch 28/100000\n",
      "255098/255098 [==============================] - 13s 50us/step - loss: 0.2272 - acc: 0.6330 - val_loss: 0.2267 - val_acc: 0.6345\n",
      "Epoch 29/100000\n",
      "255098/255098 [==============================] - 13s 49us/step - loss: 0.2271 - acc: 0.6333 - val_loss: 0.2273 - val_acc: 0.6333\n",
      "Epoch 30/100000\n",
      "255098/255098 [==============================] - 13s 52us/step - loss: 0.2272 - acc: 0.6335 - val_loss: 0.2262 - val_acc: 0.6349\n",
      "Epoch 31/100000\n",
      "255098/255098 [==============================] - 13s 50us/step - loss: 0.2271 - acc: 0.6332 - val_loss: 0.2266 - val_acc: 0.6343\n",
      "Epoch 32/100000\n",
      "255098/255098 [==============================] - 14s 56us/step - loss: 0.2271 - acc: 0.6335 - val_loss: 0.2273 - val_acc: 0.6338\n",
      "Epoch 33/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.2272 - acc: 0.6334 - val_loss: 0.2268 - val_acc: 0.6340\n",
      "Epoch 34/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.2271 - acc: 0.6325 - val_loss: 0.2266 - val_acc: 0.6358\n",
      "Epoch 35/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.2271 - acc: 0.6337 - val_loss: 0.2262 - val_acc: 0.6347\n",
      "Epoch 36/100000\n",
      "255098/255098 [==============================] - 13s 50us/step - loss: 0.2270 - acc: 0.6338 - val_loss: 0.2280 - val_acc: 0.6319\n",
      "Epoch 37/100000\n",
      "255098/255098 [==============================] - 13s 49us/step - loss: 0.2270 - acc: 0.6330 - val_loss: 0.2267 - val_acc: 0.6364\n",
      "Epoch 38/100000\n",
      "255098/255098 [==============================] - 13s 51us/step - loss: 0.2272 - acc: 0.6325 - val_loss: 0.2264 - val_acc: 0.6375\n",
      "Epoch 39/100000\n",
      "255098/255098 [==============================] - 13s 51us/step - loss: 0.2269 - acc: 0.6339 - val_loss: 0.2271 - val_acc: 0.6345\n",
      "Epoch 40/100000\n",
      "255098/255098 [==============================] - 13s 51us/step - loss: 0.2271 - acc: 0.6329 - val_loss: 0.2266 - val_acc: 0.6343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c611a30550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treinando ensemble mlps de modelos heterogêneos\n",
    "mlp1 = models.Sequential()\n",
    "mlp1.add(layers.Dense(100, activation='relu', input_dim=input_dim, \n",
    "                      kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp1.add(layers.Dense(50, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)))\n",
    "mlp1.add(layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)))\n",
    "mlp1.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp1.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "mlp1.fit(X_train1, Y_train1, batch_size=64, epochs=100000,\n",
    "                         callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val1, Y_val1))\n",
    "\n",
    "mlp2 = models.Sequential()\n",
    "mlp2.add(layers.Dense(128, activation='tanh', input_dim=input_dim, \n",
    "                      kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp2.add(layers.Dense(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.005)))\n",
    "mlp2.add(layers.Dense(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.005)))\n",
    "mlp2.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "mlp2.fit(X_train2, Y_train2, batch_size=64, epochs=100000,\n",
    "                        callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val2, Y_val2))\n",
    "\n",
    "mlp3 = models.Sequential()\n",
    "mlp3.add(layers.Dense(100, activation='tanh', input_dim=input_dim,\n",
    "                     kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp3.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp3.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "mlp3.fit(X_train3, Y_train3, batch_size=64, epochs=100000,\n",
    "                         callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val3, Y_val3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 26s 101us/step - loss: 0.6901 - acc: 0.6222 - val_loss: 0.6510 - val_acc: 0.6327\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 19s 76us/step - loss: 0.6513 - acc: 0.6303 - val_loss: 0.6465 - val_acc: 0.6336\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 21s 84us/step - loss: 0.6476 - acc: 0.6330 - val_loss: 0.6463 - val_acc: 0.6339\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 18s 70us/step - loss: 0.6461 - acc: 0.6337 - val_loss: 0.6477 - val_acc: 0.6329\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.6456 - acc: 0.6343 - val_loss: 0.6487 - val_acc: 0.6291\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 17s 67us/step - loss: 0.6453 - acc: 0.6344 - val_loss: 0.6433 - val_acc: 0.6361\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 18s 72us/step - loss: 0.6447 - acc: 0.6358 - val_loss: 0.6483 - val_acc: 0.6308\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 19s 73us/step - loss: 0.6445 - acc: 0.6349 - val_loss: 0.6439 - val_acc: 0.6370\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 20s 76us/step - loss: 0.6441 - acc: 0.6349 - val_loss: 0.6489 - val_acc: 0.6277\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 21s 80us/step - loss: 0.6441 - acc: 0.6350 - val_loss: 0.6486 - val_acc: 0.6321\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 19s 75us/step - loss: 0.6440 - acc: 0.6352 - val_loss: 0.6439 - val_acc: 0.6339\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 17s 66us/step - loss: 0.6438 - acc: 0.6354 - val_loss: 0.6463 - val_acc: 0.6337\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.6439 - acc: 0.6351 - val_loss: 0.6429 - val_acc: 0.6341\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 17s 66us/step - loss: 0.6438 - acc: 0.6353 - val_loss: 0.6445 - val_acc: 0.6346\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 19s 75us/step - loss: 0.6440 - acc: 0.6352 - val_loss: 0.6429 - val_acc: 0.6370\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 18s 72us/step - loss: 0.6440 - acc: 0.6350 - val_loss: 0.6496 - val_acc: 0.6239\n",
      "Epoch 17/100000\n",
      "255098/255098 [==============================] - 17s 68us/step - loss: 0.6436 - acc: 0.6357 - val_loss: 0.6423 - val_acc: 0.6365\n",
      "Epoch 18/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.6437 - acc: 0.6354 - val_loss: 0.6435 - val_acc: 0.6342\n",
      "Epoch 19/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6435 - acc: 0.6354 - val_loss: 0.6432 - val_acc: 0.6360\n",
      "Epoch 20/100000\n",
      "255098/255098 [==============================] - 18s 69us/step - loss: 0.6436 - acc: 0.6355 - val_loss: 0.6422 - val_acc: 0.6376\n",
      "Epoch 21/100000\n",
      "255098/255098 [==============================] - 18s 72us/step - loss: 0.6437 - acc: 0.6363 - val_loss: 0.6445 - val_acc: 0.6311\n",
      "Epoch 22/100000\n",
      "255098/255098 [==============================] - 20s 79us/step - loss: 0.6436 - acc: 0.6357 - val_loss: 0.6428 - val_acc: 0.6366\n",
      "Epoch 23/100000\n",
      "255098/255098 [==============================] - 19s 74us/step - loss: 0.6433 - acc: 0.6357 - val_loss: 0.6432 - val_acc: 0.6352\n",
      "Epoch 24/100000\n",
      "255098/255098 [==============================] - 19s 73us/step - loss: 0.6435 - acc: 0.6354 - val_loss: 0.6420 - val_acc: 0.6365\n",
      "Epoch 25/100000\n",
      "255098/255098 [==============================] - 18s 69us/step - loss: 0.6435 - acc: 0.6358 - val_loss: 0.6426 - val_acc: 0.6367\n",
      "Epoch 26/100000\n",
      "255098/255098 [==============================] - 20s 80us/step - loss: 0.6435 - acc: 0.6362 - val_loss: 0.6439 - val_acc: 0.6359\n",
      "Epoch 27/100000\n",
      "255098/255098 [==============================] - 18s 69us/step - loss: 0.6436 - acc: 0.6355 - val_loss: 0.6442 - val_acc: 0.6346\n",
      "Epoch 28/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.6433 - acc: 0.6365 - val_loss: 0.6432 - val_acc: 0.6368\n",
      "Epoch 29/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.6433 - acc: 0.6354 - val_loss: 0.6435 - val_acc: 0.6341\n",
      "Epoch 30/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6432 - acc: 0.6366 - val_loss: 0.6431 - val_acc: 0.6376\n",
      "Epoch 31/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.6433 - acc: 0.6353 - val_loss: 0.6440 - val_acc: 0.6356\n",
      "Epoch 32/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6434 - acc: 0.6361 - val_loss: 0.6454 - val_acc: 0.6354\n",
      "Epoch 33/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6432 - acc: 0.6356 - val_loss: 0.6449 - val_acc: 0.6334\n",
      "Epoch 34/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.6433 - acc: 0.6357 - val_loss: 0.6420 - val_acc: 0.6370\n",
      "Epoch 35/100000\n",
      "255098/255098 [==============================] - 17s 66us/step - loss: 0.6431 - acc: 0.6350 - val_loss: 0.6452 - val_acc: 0.6296\n",
      "Epoch 36/100000\n",
      "255098/255098 [==============================] - 17s 68us/step - loss: 0.6432 - acc: 0.6355 - val_loss: 0.6426 - val_acc: 0.6336\n",
      "Epoch 37/100000\n",
      "255098/255098 [==============================] - 18s 69us/step - loss: 0.6434 - acc: 0.6351 - val_loss: 0.6500 - val_acc: 0.6303\n",
      "Epoch 38/100000\n",
      "255098/255098 [==============================] - 17s 68us/step - loss: 0.6431 - acc: 0.6348 - val_loss: 0.6415 - val_acc: 0.6362\n",
      "Epoch 39/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.6434 - acc: 0.6352 - val_loss: 0.6417 - val_acc: 0.6366\n",
      "Epoch 40/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6433 - acc: 0.6351 - val_loss: 0.6421 - val_acc: 0.6345\n",
      "Epoch 41/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.6431 - acc: 0.6351 - val_loss: 0.6420 - val_acc: 0.6347\n",
      "Epoch 42/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.6429 - acc: 0.6349 - val_loss: 0.6438 - val_acc: 0.6347\n",
      "Epoch 43/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6434 - acc: 0.6351 - val_loss: 0.6435 - val_acc: 0.6350\n",
      "Epoch 44/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6429 - acc: 0.6358 - val_loss: 0.6422 - val_acc: 0.6363\n",
      "Epoch 45/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.6429 - acc: 0.6360 - val_loss: 0.6440 - val_acc: 0.6345\n",
      "Epoch 46/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6431 - acc: 0.6349 - val_loss: 0.6448 - val_acc: 0.6327\n",
      "Epoch 47/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.6429 - acc: 0.6348 - val_loss: 0.6417 - val_acc: 0.6354\n",
      "Epoch 48/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.6430 - acc: 0.6355 - val_loss: 0.6419 - val_acc: 0.6360\n",
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 23s 91us/step - loss: 0.2569 - acc: 0.6170 - val_loss: 0.2333 - val_acc: 0.6221\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 18s 71us/step - loss: 0.2322 - acc: 0.6252 - val_loss: 0.2321 - val_acc: 0.6221\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 18s 72us/step - loss: 0.2308 - acc: 0.6270 - val_loss: 0.2298 - val_acc: 0.6292\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 22s 86us/step - loss: 0.2302 - acc: 0.6287 - val_loss: 0.2290 - val_acc: 0.6308\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 20s 78us/step - loss: 0.2296 - acc: 0.6301 - val_loss: 0.2298 - val_acc: 0.6321\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 18s 71us/step - loss: 0.2294 - acc: 0.6308 - val_loss: 0.2289 - val_acc: 0.6320\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 21s 83us/step - loss: 0.2292 - acc: 0.6321 - val_loss: 0.2282 - val_acc: 0.6339\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 19s 75us/step - loss: 0.2291 - acc: 0.6311 - val_loss: 0.2286 - val_acc: 0.6334\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 20s 77us/step - loss: 0.2289 - acc: 0.6326 - val_loss: 0.2278 - val_acc: 0.6362\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 28s 110us/step - loss: 0.2288 - acc: 0.6319 - val_loss: 0.2278 - val_acc: 0.6343\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 20s 79us/step - loss: 0.2289 - acc: 0.6324 - val_loss: 0.2288 - val_acc: 0.6346\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 22s 88us/step - loss: 0.2288 - acc: 0.6328 - val_loss: 0.2288 - val_acc: 0.6328\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 24s 95us/step - loss: 0.2288 - acc: 0.6329 - val_loss: 0.2299 - val_acc: 0.6336\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 27s 106us/step - loss: 0.2284 - acc: 0.6329 - val_loss: 0.2276 - val_acc: 0.6359\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 18s 70us/step - loss: 0.2287 - acc: 0.6335 - val_loss: 0.2278 - val_acc: 0.6342\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 18s 71us/step - loss: 0.2284 - acc: 0.6330 - val_loss: 0.2288 - val_acc: 0.6348\n",
      "Epoch 17/100000\n",
      "255098/255098 [==============================] - 21s 83us/step - loss: 0.2286 - acc: 0.6319 - val_loss: 0.2368 - val_acc: 0.6161\n",
      "Epoch 18/100000\n",
      "255098/255098 [==============================] - 21s 83us/step - loss: 0.2287 - acc: 0.6322 - val_loss: 0.2278 - val_acc: 0.6359\n",
      "Epoch 19/100000\n",
      "255098/255098 [==============================] - 21s 84us/step - loss: 0.2286 - acc: 0.6327 - val_loss: 0.2300 - val_acc: 0.6306\n",
      "Epoch 20/100000\n",
      "255098/255098 [==============================] - 28s 109us/step - loss: 0.2285 - acc: 0.6329 - val_loss: 0.2275 - val_acc: 0.6356\n",
      "Epoch 21/100000\n",
      "255098/255098 [==============================] - 19s 75us/step - loss: 0.2286 - acc: 0.6317 - val_loss: 0.2280 - val_acc: 0.6349\n",
      "Epoch 22/100000\n",
      "255098/255098 [==============================] - 23s 89us/step - loss: 0.2285 - acc: 0.6331 - val_loss: 0.2292 - val_acc: 0.6332\n",
      "Epoch 23/100000\n",
      "255098/255098 [==============================] - 23s 90us/step - loss: 0.2283 - acc: 0.6329 - val_loss: 0.2280 - val_acc: 0.6341\n",
      "Epoch 24/100000\n",
      "255098/255098 [==============================] - 22s 86us/step - loss: 0.2284 - acc: 0.6331 - val_loss: 0.2274 - val_acc: 0.6353\n",
      "Epoch 25/100000\n",
      "255098/255098 [==============================] - 22s 85us/step - loss: 0.2284 - acc: 0.6338 - val_loss: 0.2277 - val_acc: 0.6365\n",
      "Epoch 26/100000\n",
      "255098/255098 [==============================] - 19s 74us/step - loss: 0.2285 - acc: 0.6324 - val_loss: 0.2309 - val_acc: 0.6284\n",
      "Epoch 27/100000\n",
      "255098/255098 [==============================] - 19s 76us/step - loss: 0.2282 - acc: 0.6334 - val_loss: 0.2282 - val_acc: 0.6335\n",
      "Epoch 28/100000\n",
      "255098/255098 [==============================] - 22s 87us/step - loss: 0.2284 - acc: 0.6327 - val_loss: 0.2275 - val_acc: 0.6352\n",
      "Epoch 29/100000\n",
      "255098/255098 [==============================] - 24s 93us/step - loss: 0.2284 - acc: 0.6332 - val_loss: 0.2280 - val_acc: 0.6330\n",
      "Epoch 30/100000\n",
      "255098/255098 [==============================] - 19s 75us/step - loss: 0.2284 - acc: 0.6339 - val_loss: 0.2287 - val_acc: 0.6343\n",
      "Epoch 31/100000\n",
      "255098/255098 [==============================] - 20s 76us/step - loss: 0.2283 - acc: 0.6331 - val_loss: 0.2296 - val_acc: 0.6306\n",
      "Epoch 32/100000\n",
      "255098/255098 [==============================] - 20s 80us/step - loss: 0.2284 - acc: 0.6329 - val_loss: 0.2284 - val_acc: 0.6347\n",
      "Epoch 33/100000\n",
      "255098/255098 [==============================] - 21s 83us/step - loss: 0.2284 - acc: 0.6332 - val_loss: 0.2286 - val_acc: 0.6354\n",
      "Epoch 34/100000\n",
      "255098/255098 [==============================] - 18s 71us/step - loss: 0.2284 - acc: 0.6331 - val_loss: 0.2276 - val_acc: 0.6354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c612552c18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treinando ensemble mlps de modelos heterogêneos\n",
    "mlp4 = models.Sequential()\n",
    "mlp4.add(layers.Dense(128, activation='relu', input_dim=input_dim, \n",
    "                      kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp4.add(layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp4.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp4.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mlp4.fit(X_train4, Y_train4, batch_size=64, epochs=100000,\n",
    "                         callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val4, Y_val4))\n",
    "\n",
    "mlp5 = models.Sequential()\n",
    "mlp5.add(layers.Dense(128, activation='tanh', input_dim=input_dim, \n",
    "                      kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp5.add(layers.Dense(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp5.add(layers.Dense(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp5.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp5.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "mlp5.fit(X_train5, Y_train5, batch_size=64, epochs=100000,\n",
    "                         callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val5, Y_val5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando ensembles com 3 e 5 mlps, respectivamente\n",
    "ensemble1 = [mlp1, mlp2, mlp3]\n",
    "ensemble2 = [mlp1, mlp2, mlp3, mlp4, mlp5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usaremos a moda para pegar a classe com mais votos dos classificadores\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = [model.predict_classes(X_test, verbose=0) for model in ensemble1]\n",
    "y_pred1 = np.array(y_pred1)\n",
    "y_pred1 = stats.mode(y_pred1, axis=0).mode[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = [model.predict_classes(X_test, verbose=0) for model in ensemble2]\n",
    "y_pred2 = np.array(y_pred2)\n",
    "y_pred2 = stats.mode(y_pred2, axis=0).mode[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6334263328874451 0.6218941088739747 0.6484611915462731 0.6348998500224586\n"
     ]
    }
   ],
   "source": [
    "#Resultados para ensemble de 3 mlps\n",
    "print(utils.accuracy_score(Y_test, y_pred1),\n",
    "utils.recall_score(Y_test, y_pred1),\n",
    "utils.precision_score(Y_test, y_pred1),\n",
    "utils.f1_score(Y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6372100133766482 0.6572110365398957 0.6428863211951448 0.6499697626738647\n"
     ]
    }
   ],
   "source": [
    "#Resultados para ensemble de 5 mlps\n",
    "print(utils.accuracy_score(Y_test, y_pred2),\n",
    "utils.recall_score(Y_test, y_pred2),\n",
    "utils.precision_score(Y_test, y_pred2),\n",
    "utils.f1_score(Y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 21s 82us/step - loss: 0.6768 - acc: 0.6227 - val_loss: 0.6549 - val_acc: 0.6273\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6484 - acc: 0.6339 - val_loss: 0.6508 - val_acc: 0.6283\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6442 - acc: 0.6354 - val_loss: 0.6477 - val_acc: 0.6311\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6434 - acc: 0.6365 - val_loss: 0.6472 - val_acc: 0.6318\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6425 - acc: 0.6373 - val_loss: 0.6438 - val_acc: 0.6354\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6424 - acc: 0.6369 - val_loss: 0.6437 - val_acc: 0.6346\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6419 - acc: 0.6372 - val_loss: 0.6465 - val_acc: 0.6330\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6417 - acc: 0.6370 - val_loss: 0.6442 - val_acc: 0.6345\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6418 - acc: 0.6373 - val_loss: 0.6463 - val_acc: 0.6308\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6418 - acc: 0.6362 - val_loss: 0.6491 - val_acc: 0.6281\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6413 - acc: 0.6371 - val_loss: 0.6469 - val_acc: 0.6300\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6417 - acc: 0.6372 - val_loss: 0.6436 - val_acc: 0.6338\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6416 - acc: 0.6370 - val_loss: 0.6438 - val_acc: 0.6347\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6414 - acc: 0.6372 - val_loss: 0.6446 - val_acc: 0.6335\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6415 - acc: 0.6374 - val_loss: 0.6444 - val_acc: 0.6341\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6415 - acc: 0.6366 - val_loss: 0.6428 - val_acc: 0.6359\n",
      "Epoch 17/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6413 - acc: 0.6378 - val_loss: 0.6432 - val_acc: 0.6347\n",
      "Epoch 18/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6412 - acc: 0.6374 - val_loss: 0.6444 - val_acc: 0.6328\n",
      "Epoch 19/100000\n",
      "255098/255098 [==============================] - 16s 64us/step - loss: 0.6413 - acc: 0.6371 - val_loss: 0.6438 - val_acc: 0.6342\n",
      "Epoch 20/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6412 - acc: 0.6379 - val_loss: 0.6452 - val_acc: 0.6330\n",
      "Epoch 21/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6413 - acc: 0.6372 - val_loss: 0.6457 - val_acc: 0.6307\n",
      "Epoch 22/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6411 - acc: 0.6381 - val_loss: 0.6434 - val_acc: 0.6346\n",
      "Epoch 23/100000\n",
      "255098/255098 [==============================] - 17s 66us/step - loss: 0.6408 - acc: 0.6379 - val_loss: 0.6435 - val_acc: 0.6338\n",
      "Epoch 24/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6410 - acc: 0.6375 - val_loss: 0.6454 - val_acc: 0.6340\n",
      "Epoch 25/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6409 - acc: 0.6373 - val_loss: 0.6423 - val_acc: 0.6354\n",
      "Epoch 26/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6411 - acc: 0.6373 - val_loss: 0.6539 - val_acc: 0.6232\n",
      "Epoch 27/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6410 - acc: 0.6377 - val_loss: 0.6442 - val_acc: 0.6342\n",
      "Epoch 28/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6409 - acc: 0.6368 - val_loss: 0.6440 - val_acc: 0.6341\n",
      "Epoch 29/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6410 - acc: 0.6366 - val_loss: 0.6486 - val_acc: 0.6306\n",
      "Epoch 30/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6407 - acc: 0.6376 - val_loss: 0.6444 - val_acc: 0.6326\n",
      "Epoch 31/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6409 - acc: 0.6371 - val_loss: 0.6492 - val_acc: 0.6278\n",
      "Epoch 32/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6412 - acc: 0.6375 - val_loss: 0.6450 - val_acc: 0.6340\n",
      "Epoch 33/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6412 - acc: 0.6370 - val_loss: 0.6482 - val_acc: 0.6327\n",
      "Epoch 34/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6410 - acc: 0.6373 - val_loss: 0.6437 - val_acc: 0.6352\n",
      "Epoch 35/100000\n",
      "255098/255098 [==============================] - 14s 57us/step - loss: 0.6409 - acc: 0.6373 - val_loss: 0.6444 - val_acc: 0.6342\n",
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 18s 69us/step - loss: 0.6753 - acc: 0.6214 - val_loss: 0.6542 - val_acc: 0.6268\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6512 - acc: 0.6294 - val_loss: 0.6474 - val_acc: 0.6341\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6473 - acc: 0.6328 - val_loss: 0.6466 - val_acc: 0.6340\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6460 - acc: 0.6338 - val_loss: 0.6484 - val_acc: 0.6316\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6455 - acc: 0.6336 - val_loss: 0.6478 - val_acc: 0.6308\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6448 - acc: 0.6349 - val_loss: 0.6473 - val_acc: 0.6324\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6442 - acc: 0.6352 - val_loss: 0.6451 - val_acc: 0.6333\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6438 - acc: 0.6344 - val_loss: 0.6503 - val_acc: 0.6255\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6436 - acc: 0.6354 - val_loss: 0.6445 - val_acc: 0.6343\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 15s 61us/step - loss: 0.6434 - acc: 0.6357 - val_loss: 0.6438 - val_acc: 0.6353\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6434 - acc: 0.6346 - val_loss: 0.6430 - val_acc: 0.6360\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6430 - acc: 0.6354 - val_loss: 0.6446 - val_acc: 0.6337\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6430 - acc: 0.6356 - val_loss: 0.6482 - val_acc: 0.6299\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.6432 - acc: 0.6350 - val_loss: 0.6443 - val_acc: 0.6350\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6429 - acc: 0.6351 - val_loss: 0.6484 - val_acc: 0.6285\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6426 - acc: 0.6360 - val_loss: 0.6421 - val_acc: 0.6370\n",
      "Epoch 17/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6430 - acc: 0.6341 - val_loss: 0.6434 - val_acc: 0.6378\n",
      "Epoch 18/100000\n",
      "255098/255098 [==============================] - 17s 65us/step - loss: 0.6424 - acc: 0.6358 - val_loss: 0.6432 - val_acc: 0.6368\n",
      "Epoch 19/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6426 - acc: 0.6353 - val_loss: 0.6455 - val_acc: 0.6347\n",
      "Epoch 20/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6424 - acc: 0.6352 - val_loss: 0.6421 - val_acc: 0.6369\n",
      "Epoch 21/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6422 - acc: 0.6356 - val_loss: 0.6425 - val_acc: 0.6364\n",
      "Epoch 22/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6426 - acc: 0.6348 - val_loss: 0.6435 - val_acc: 0.6333\n",
      "Epoch 23/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6428 - acc: 0.6346 - val_loss: 0.6433 - val_acc: 0.6355\n",
      "Epoch 24/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6427 - acc: 0.6348 - val_loss: 0.6431 - val_acc: 0.6354\n",
      "Epoch 25/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6425 - acc: 0.6352 - val_loss: 0.6448 - val_acc: 0.6339\n",
      "Epoch 26/100000\n",
      "255098/255098 [==============================] - 15s 60us/step - loss: 0.6427 - acc: 0.6344 - val_loss: 0.6430 - val_acc: 0.6365\n",
      "Train on 255098 samples, validate on 127548 samples\n",
      "Epoch 1/100000\n",
      "255098/255098 [==============================] - 19s 74us/step - loss: 0.6736 - acc: 0.6206 - val_loss: 0.6517 - val_acc: 0.6318\n",
      "Epoch 2/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6490 - acc: 0.6315 - val_loss: 0.6459 - val_acc: 0.6355\n",
      "Epoch 3/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6459 - acc: 0.6324 - val_loss: 0.6444 - val_acc: 0.6365\n",
      "Epoch 4/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6448 - acc: 0.6332 - val_loss: 0.6504 - val_acc: 0.6265\n",
      "Epoch 5/100000\n",
      "255098/255098 [==============================] - 16s 61us/step - loss: 0.6443 - acc: 0.6337 - val_loss: 0.6460 - val_acc: 0.6337\n",
      "Epoch 6/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6435 - acc: 0.6346 - val_loss: 0.6434 - val_acc: 0.6353\n",
      "Epoch 7/100000\n",
      "255098/255098 [==============================] - 15s 57us/step - loss: 0.6434 - acc: 0.6339 - val_loss: 0.6435 - val_acc: 0.6365\n",
      "Epoch 8/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6432 - acc: 0.6343 - val_loss: 0.6455 - val_acc: 0.6333\n",
      "Epoch 9/100000\n",
      "255098/255098 [==============================] - 16s 63us/step - loss: 0.6429 - acc: 0.6343 - val_loss: 0.6432 - val_acc: 0.6353\n",
      "Epoch 10/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6427 - acc: 0.6351 - val_loss: 0.6430 - val_acc: 0.6359\n",
      "Epoch 11/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6428 - acc: 0.6337 - val_loss: 0.6413 - val_acc: 0.6382\n",
      "Epoch 12/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6429 - acc: 0.6345 - val_loss: 0.6427 - val_acc: 0.6366\n",
      "Epoch 13/100000\n",
      "255098/255098 [==============================] - 16s 62us/step - loss: 0.6425 - acc: 0.6340 - val_loss: 0.6420 - val_acc: 0.6374\n",
      "Epoch 14/100000\n",
      "255098/255098 [==============================] - 17s 68us/step - loss: 0.6424 - acc: 0.6348 - val_loss: 0.6426 - val_acc: 0.6365\n",
      "Epoch 15/100000\n",
      "255098/255098 [==============================] - 15s 59us/step - loss: 0.6425 - acc: 0.6344 - val_loss: 0.6428 - val_acc: 0.6352\n",
      "Epoch 16/100000\n",
      "255098/255098 [==============================] - 15s 58us/step - loss: 0.6426 - acc: 0.6350 - val_loss: 0.6432 - val_acc: 0.6353\n",
      "Epoch 17/100000\n",
      "254720/255098 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.6349"
     ]
    }
   ],
   "source": [
    "#Treinando ensemble de mlps de modelos homogêneos\n",
    "mlp6_1 = models.Sequential()\n",
    "mlp6_1.add(layers.Dense(50, activation='relu', input_dim=input_dim, \n",
    "                      kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp6_1.add(layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)))\n",
    "mlp6_1.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp6_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mlp6_1.fit(X_train1, Y_train1, batch_size=64, epochs=100000,\n",
    "                         callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val1, Y_val1))\n",
    "\n",
    "mlp6_2 = models.Sequential()\n",
    "mlp6_2.add(layers.Dense(50, activation='relu', input_dim=input_dim, \n",
    "                      kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp6_2.add(layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)))\n",
    "mlp6_2.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp6_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mlp6_2.fit(X_train2, Y_train2, batch_size=64, epochs=100000,\n",
    "                         callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val2, Y_val2))\n",
    "\n",
    "mlp6_3 = models.Sequential()\n",
    "mlp6_3.add(layers.Dense(50, activation='relu', input_dim=input_dim, \n",
    "                      kernel_initializer=tf.keras.initializers.he_normal(seed=None),\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp6_3.add(layers.Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "mlp6_3.add(layers.Dense(1, activation='sigmoid'))\n",
    "mlp6_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mlp6_3.fit(X_train3, Y_train3, batch_size=64, epochs=100000,\n",
    "                         callbacks=[callbacks.EarlyStopping(patience=10)], validation_data=(X_val3, Y_val3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6387846359640742 0.6618941088739746 0.6435054954615318 0.6525702858487237\n"
     ]
    }
   ],
   "source": [
    "ensemble3 = [mlp6_1, mlp6_2, mlp6_3]\n",
    "\n",
    "y_pred3 = [model.predict_classes(X_test, verbose=0) for model in ensemble3]\n",
    "y_pred3 = np.array(y_pred3)\n",
    "y_pred3 = stats.mode(y_pred3, axis=0).mode[0] \n",
    "\n",
    "print(utils.accuracy_score(Y_test, y_pred3),\n",
    "utils.recall_score(Y_test, y_pred3),\n",
    "utils.precision_score(Y_test, y_pred3),\n",
    "utils.f1_score(Y_test, y_pred3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
